{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性可分SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题引入：\n",
    "\n",
    "假如数据集满足线性可分，设分割超平面为$wx+b=0$，此时，有无数条分割超平面满足将样本分开，但此时哪条最好呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数间隔\n",
    "不知道怎么插图。。。\n",
    "\n",
    "但可以想象，肯定是不但能分割，而且离两类都最远的那个平面。\n",
    "\n",
    "因此，用$wx_i+b$可以度量点$x_i$到分割超平面的距离，如果再乘上$y_i$，则可以表示它的类别，即可以定义：\n",
    "$$\n",
    "    \\hat{\\gamma_i} = y_i*(wx_i+b)\n",
    "$$\n",
    "称为函数间隔。\n",
    "\n",
    "则我们的目标为求得$\\hat{\\gamma} = \\min\\hat{\\gamma_i}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几何间隔\n",
    "\n",
    "但函数间隔有一个问题，就是如果$w$成倍的增加或者缩小，其实并不会影响分割超平面的位置，但$\\hat{\\gamma_i}$则会改变，因此我们可以对$w$进行约束，即得到几何间隔（想想中学学的点到直线距离，分母都除了一个约束)，定义为：\n",
    "$$\n",
    "\\gamma_i = \\frac{w}{\\left\\|w\\right\\|}x+\\frac{b}{\\left\\|w\\right\\|}\n",
    "$$\n",
    "\n",
    "则问题变为求解$\\gamma = \\min \\gamma_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数间隔与几何间隔的关系\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{\\hat{\\gamma}}{\\left\\|w\\right\\|}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了几何间隔和函数间隔的定义，我们现在回到最开始的问题，寻找最好的分割超平面就变成了能满足最大几何间隔的超平面,由几何间隔和函数间隔的关系，问题定义为\n",
    "\n",
    "$$\n",
    "\\max \\frac{\\hat{\\gamma}}{\\left\\|w\\right\\|}\n",
    "$$\n",
    "$s.t. y_i(wx_i+b)\\ge\\hat\\gamma$ 为约束条件，即首先得满足$\\hat\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，函数间隔$\\hat\\gamma$的取值并不会影响超平面的位置，即$\\hat\\gamma$和$\\lambda\\hat\\gamma$得到的超平面相同，故我们可取$\\hat\\gamma =1$，这样，问题就变为\n",
    "$$\n",
    "    \\max \\frac{1}{\\left\\|w\\right\\|}\n",
    "$$\n",
    "$s.t. y_i(wx_i+b)\\ge\\hat\\gamma = 1$ ,我们将最大转化为最小，同时加上便于求导的系数$\\frac{1}{2}$，线性SVM的目标可写成：\n",
    "$$\n",
    "    \\min\\frac{1}{2}\\left\\|w\\right\\|^2\n",
    "$$\n",
    "$s.t. y_i(wx_i+b)-1\\ge 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于求解上述问题，一般都根据拉格朗日对偶性，通过求解对偶问题来解决。使用该方法主要有两个好处\n",
    "１．往往更容易求解\n",
    "２．便于引入核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求解\n",
    "\n",
    "至于什么是拉格朗日对偶函数，本篇不做介绍了。\n",
    "\n",
    "继续讲SVM\n",
    "\n",
    "我们先引入拉格朗日函数，定义为：\n",
    "$$\n",
    "L(w,\\alpha,b) = \\frac{1}{2}\\left\\|w\\right\\|^2 - \\sum^{N}_{i=1}\\alpha_iy_i(wx_i+b)+\\sum^{N}_{i=1}\\alpha_i\n",
    "$$\n",
    "$s.t. y_i(wx_i+b)-1\\ge 0$ \n",
    "\n",
    "$\\alpha = (\\alpha_1,\\alpha_2,\\cdots,\\alpha_n)^T$为拉格朗日乘子\n",
    "\n",
    "则原始问题的对偶问题是极大极小问题\n",
    "\n",
    "$$\n",
    " \\max_\\alpha\\min_{w,b}L(w,\\alpha,b)\n",
    "$$\n",
    "\n",
    "故先求极小，后求极大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１．求$\\min_{w,b}L(w,\\alpha,\\beta)$\n",
    "\n",
    "$\\nabla_w L(w,\\alpha,b)= w-\\sum^{N}_{i=1}\\alpha_iy_ix_i = 0$\n",
    "\n",
    "$\\nabla_b L(w,\\alpha,b) = \\sum^{N}_{i=1}\\alpha_iy_i = 0$\n",
    "\n",
    "得：$w = \\sum^{N}_{i=1}\\alpha_iy_ix_i$\n",
    "\n",
    "  $\\sum^{N}_{i=1}\\alpha_iy_i=0$\n",
    "  \n",
    " 将结果带回拉格朗日函数，得：\n",
    " $$\n",
    " L(w,\\alpha,b) = \\frac{1}{2}\\sum^{N}_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i*x_j)-\\sum^N_{i=1}\\alpha_iy_i((\\sum^N_{j=1}\\alpha_jy_jx_j)*x_i+b)+\\sum^N_{i=1}\\alpha_i = -\\frac{1}{2}\\sum^{N}_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i*x_j)+\\sum^N_{i=1}\\alpha_i \n",
    " $$\n",
    " \n",
    " ２．求$\\min_{\\alpha}L(w,\\alpha,b)$对$\\alpha$的极大，即是对偶问题，\n",
    " \n",
    " $$\n",
    "     \\max_\\alpha-\\frac{1}{2}\\sum^{N}_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i*x_j)+\\sum^N_{i=1}\\alpha_i \n",
    " $$\n",
    " $s.t.\\sum^N_{i=1}\\alpha_iy_i=0$\n",
    " \n",
    " $\\alpha_i\\ge0,i=1,2,\\cdots,N$\n",
    " \n",
    " 将上式转化为求极小，就得到下边与之等价的对偶最优化问题：\n",
    " $$\n",
    "     \\min_\\alpha \\frac{1}{2}\\sum^{N}_{i=1}\\sum^N_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i*x_j)-\\sum^N_{i=1}\\alpha_i \n",
    " $$\n",
    "  $s.t.\\sum^N_{i=1}\\alpha_iy_i=0$\n",
    " \n",
    " $\\alpha_i\\ge0,i=1,2,\\cdots,N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式可求得对偶问题的解$\\alpha^*$,则将$\\alpha^*$带入KKT条件，可求得$w^*,b^*$，它也是原始问题的解\n",
    "\n",
    "KKT条件：\n",
    "\n",
    "$\\nabla_wL(w^*,\\alpha^*,b^*) = w^*-\\sum^N_{i=1}\\alpha^*_iy_ix_i = 0$\n",
    "\n",
    "$\\nabla_wL(w^*,\\alpha^*,b^*) = -\\sum^N_{i=1}\\alpha^*_iy_i=0$\n",
    "\n",
    "$\\alpha^*_i(y_i(w^*x_i+b^*)-1) = 0$\n",
    "\n",
    "$y_i(w^*x_i+b^*)-1\\ge0$\n",
    "\n",
    "$\\alpha^*_i\\ge0,i=1,2,\\cdots,N$\n",
    "\n",
    "由此得$w^* = \\sum^N_{i=1}\\alpha^*_iy_ix_i$,$b^*=y_j\\sum^N_{i=1}\\alpha^*_iy_i(x_i*x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由结果知，$w^*,b^*$只依赖于$\\alpha^*_i>0$的样本点，而其他样本点没有影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们就得到了我们想要的结果，至于最后的那个最小化问题如何求，统计学习方法中给出的是使用序列最小最优化算法，这个不想看了，大致思路就是每次取两个变量，固定其他变量然后求二次规划问题。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性不可分数据，我们可以容忍一些点进入margin，因此，原约束$y_i(wx_i+b)\\ge1$可以稍稍修改，使条件放宽一些，即加入一个松弛变量$\\xi_i$,得到新的约束$y_i(wx_i+b)\\ge1-\\xi_i$,然后对于每个松弛变量$\\xi_i$，支付一个代价$\\xi_i$，目标函数变为：\n",
    "$$\n",
    "\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum^N_{i=1}\\xi_i\n",
    "$$\n",
    "这里$C>0$称为惩罚系数，其实就是为了调节两部分的权重，C大时对误分类的惩罚增大，反之。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于上式的求解，和线性可分SVM基本一样，就不写了，求到最后会发现原本的约束$\\alpha_i>0$变成了$0<\\alpha_i<C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss\n",
    "\n",
    "观察公式$\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum^N_{i=1}\\xi_i$,与经常提到的SVM损失函数Hinge损失$\\sum^N_{i=1}[1-y_i(wx_i+b)]_++\\lambda\\left\\|w\\right\\|^2$\n",
    "\n",
    "\n",
    "其中下标$+$表示以下取正值的函数\n",
    "$$\n",
    "[z]_+ =\n",
    "\\begin{cases} \n",
    "z,  & z>0 \\\\\n",
    "0, & z\\le0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其实这俩公式是等价的。可以相互推到得到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核技巧\n",
    "\n",
    "这里不想做太多的解释了，个人的大致理解就是线性解决不了的，我们可以通过映射，将数据变为非线性的，然后再解决试试。可以想想一个转化函数$\\phi$, 核函数满足$K(x,z) = \\phi(x)\\phi(z)$\n",
    "\n",
    "在实际求解中，我们并不关心具体的$\\phi$映射函数，而且事实上它也不唯一，所以我们只需要定义它变换后的结果$K$核函数，就能达到我们想要的结果，观察核函数的构成，会发现它其实只是两个变量的内积形式，这与之前求解对偶问题最后那个优化问题公式中的形式一致，因此它能够完美的应用到SVM里边去。\n",
    "\n",
    "常用的核函数有多项式核、高斯核、线性核等等，核函数的要求是它对应的Gram矩阵为半正定矩阵\n",
    "\n",
    "\n",
    "\n",
    "### 关于核函数\n",
    "\n",
    "其实并不是只有SVM才有核函数，其实核函数思想同样可以用在其他模型里，但可以发现，在SVM中，由于有约束的限制，它的计算量其实会小很多，这可能也是SVM广泛应用核函数而其他模型没那么多用的原因吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
